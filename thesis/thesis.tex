\documentclass[MS, xcolor=dvipsnames]{wfuthesis}
\usepackage{mathtools,amsthm,amssymb,mathrsfs}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{enumitem}

% \usepackage[backend=biber]{biblatex}
% \addbibresource{bibliography.bib}
\usepackage[pdfpagelabels,draft,implicit=false]{hyperref}
\urlstyle{same}
% \usepackage{soul}
% \usepackage{float}
% \usepackage{enumerate}
% \usepackage{multicol}
% \usepackage{tikz}
% \usetikzlibrary{matrix,arrows}
% \usepackage[all]{xy}
% \usepackage{tikz-cd}
% \tikzcdset{every arrow/.append style = -{Latex[width=4pt,length=10pt]}}
\usepackage[defaultmono]{droidsansmono}
\usepackage{listings}
\lstset{basicstyle=\xpt\droidsansmono}
% \usepackage{fontspec}
% \usepackage{setspace}
% \setmainfont{Times New Roman}
% \usepackage{derivative}
% \usepackage{fancyvrb}
% \renewcommand{\theFancyVerbLine}{\textsuperscript{\arabic{FancyVerbLine}}}
% \usepackage{multirow}
% \usepackage{dsfont}
\usepackage{indentfirst}
% \usepackage{blindtext}
\usepackage{lipsum}

% \DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
% \DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}
% \def\bA{\mathbb{A}}
% \def\cB{\mathcal{B}}
\def\bC{\mathbb{C}}
\def\bF{\mathbb{F}}
% \def\bH{\mathbb{H}}
% \def\bI{\mathbb{I}}
% \def\cL{\mathcal{L}}
% \def\cM{\mathcal{M}}
\def\bN{\mathbb{N}}
\def\cP{\mathcal{P}}
\def\bQ{\mathbb{Q}}
\def\bR{\mathbb{R}}
% \def\cT{\mathcal{T}}
% \def\cU{\mathcal{U}}
% \def\bV{\mathbb{V}}
\def\bZ{\mathbb{Z}}
% \def\bk{\bk}
\def\sbs{\subseteq}
\def\sps{\supseteq}

% \newcommand{\upperset}[2]{\:
% \underset{
%   \text{\raisebox{1.2ex}{\smash{\scalebox{0.8}{$#1$}}}}
% }{
%   \text{\raisebox{0.2ex}{\smash{$#2$}}}
% }
% \:}
% \def\opset{\upperset{\text{\tiny{open}}} \subset}
% \def\clset{\upperset{\text{\tiny{closed}}} \subset}

% \DeclareMathOperator{\Aut}{Aut}
% \DeclareMathOperator{\Dim}{Dim}
% \DeclareMathOperator{\ev}{ev}
% \DeclareMathOperator{\GL}{GL}
% \DeclareMathOperator{\Hom}{Hom}
\DeclareMathOperator{\id}{id}
\DeclareMathOperator{\im}{im}
% \DeclareMathOperator{\Mat}{Mat}
% \DeclareMathOperator{\nullsp}{null}
% \DeclareMathOperator{\range}{range}
% \DeclareMathOperator{\SL}{SL}
% \DeclareMathOperator{\Span}{span}
% \newcommand{\LT}{\ensuremath{\text{\sc lt}}}
% \newcommand{\LM}{\ensuremath{\text{\sc lm}}}
% \newcommand{\LC}{\ensuremath{\text{\sc lc}}}
\def\and{\text{ and }}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{observation}[theorem]{Observation}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

% \def\e{\epsilon}
% \def\d{\delta}
% \def\p{\varphi}

\begin{document}
\title{A Functional Computer Algebra System for Polynomials}
\author{Thomas Meek}
\department{Mathematics}

\advisor{W. Frank Moore, Ph.D.}
\chairperson{Ellen Kirkman, Ph.D.}
\member{William Turkett, Ph.D.}

\date{April, 2023}

\maketitle

\clearpage

\acknowledgements 

\lipsum[1]

\tableofcontents

\abstract

\lipsum[2]

\chapters

\chapter{Introduction}
The theory of polynomials is a well-studied field of mathematics, rich with results which provide tools for mathematicians in various areas. Polynomial functions are among the simplest continuous functions to work with analytically or algebraically. Computationally, they are unquestionably the most important class of functions. It is then imperative to have a robust collection of software that can assist the mathematicians who study them. Many tools currently exist for this purpose, from dedicated high-level languages like Macaulay2 and Singular to libraries for general purpose languages like SageMath and SimPy. These software collections are referred to as \textit{computer algebra systems} or CAS's and while most have capabilities beyond polynomial manipulation, it is this feature that is the focus here. \par 
The implementation of these tools varies greatly. The Macaulay2 engine is written in C++\cite{M2} while SageMath is built with Python\cite{sagemath}, which in turn often calls C. Their respective interfaces often use a mixture of procedural, object-oriented, functional, and declarative design. This certainly makes sense in the case of SageMath as Python itself hybridizes these approaches. If a CAS is a package for a general purpose language like Python, it should obey the design principles of that language. For dedicated CAS's like Macaulay2 though, this design choice bears consideration. There is a tendency in modern languages to incorporate features which favor a variety of paradigms. The popularity of hybrid languages like Python and JavaScript has encouraged most newer languages like Rust and Go to follow in their footsteps. There are good reasons for this, however there are also benefits to sticking to a single paradigm. \par 
It is true that these paradigms are more a philosophical way of approaching software design than a well-defined feature of a language and given that, no language can be said to be strictly single paradigm. It is clear however that certain languages lend themselves to being more compatible with one paradigm than others. For example, Java is generally seen as a paragon of object-oriented programming. Every module contains exactly one class (with potentially many subclasses). Even the drivers are classes and must be instantiated before any program can be executed. One can use Java in a functional way, but doing so defeats the purpose of using Java. This is in contrast to a language like Python where it is difficult to escape the use of objects and classes, but higher order functions like folds and maps are used frequently, as are list comprehensions and other hallmarks of functional programming. At the same time, most programmers' first experience with Python is in writing plain procedural scripts. The freedom offered by such a language enables one to chose the most appropriate approach to the current task, but in doing so, it takes away the cohesiveness and predictability of a more \textit{pure} language like Java. \par 
Similarly to the way Java is considered a paragon of the object-oriented school, Haskell is often one of the first two languages people think of when one mentions functional programming, with the other of course being Lisp. The functional style offers many advantages over a procedural or object-oriented approach as evidenced in how much modern languages have borrowed from Haskell. Rust's type system strongly resembles that of Haskell\cite{poss2014} and modern JavaScript libraries like React  leverage functional programming more than any other paradigm. \par 
Haskell and the functional programming style are particularly well-suited to mathematics. The notion of a `function' in procedural programming is a bit of a misnomer. Mathematically, a function $f$ is a domain $X$, a codomain $Y$, and some subset of $X \times Y$ called the graph of $f$. What is the domain of the \lstinline{print()} function in Python? What is its codomain? What is its graph? These inconsistencies vanish in Haskell. A Haskell function \textit{is} a mathematical function. For this reason, mathematicians often regard the functional paradigm as the natural choice for mathematical software. What better way to determine the injectivity of a function than with a function whose injectivity can be determined?! \par 
It is in this spirit that the \lstinline{polynomial-algorithms} package was written. Taking advantage of the benefits offered by Haskell (as well as a few language extensions), this package offers a Polynomial type that is itself built on Monomial and Coefficient types. Several algorithms, most notably a modified version of Buchberger's algorithm to find a reduced Gr\"obner basis for a list of polynomials, are featured in this software. In addition, its scalability and modularity should provide mathematicians with an invaluable tool when analyzing polynomials. 

\chapter{Mathematical Background}
One thing that sets functional programming apart from imperative paradigms is its reliance on mathematics. Writing algorithms or any sufficiently complex code will always require some familiarity with mathematics, and the best and most efficient code is often written by programmers competent in mathematical reasoning. However, this quality is even more important when working in a language like Haskell. This fact may be seen just by reading documentation. A good technical writer will always write to their audience. If one is writing a UDP server in Java, it may be assumed that anyone maintaining that server will be familiar with terms like \textit{port}, \textit{buffer}, or \textit{packet}, so they may appear frequently in documentation. If one is writing an operating system in C, terms like \textit{fork} and \textit{process ID} are ubiquitous enough to appear in documentation without explanation. However, one is unlikely to encounter mentions of algebraic ring theory in the documentation for such projects. When using Haskell, this is quite common\cite{Prelude}. \par 
It is therefore imperative (pun intended) for a prospective Haskeller\footnote{The term \textit{Haskeller} refers to a regular user of Haskell.} to have a working knowledge of several areas of mathematics not usually necessary for writing programs in a procedural or object-oriented style. This is doubly important when using Haskell to write a CAS. The fact that the recommended backgrounds for the problem domain and the solution domain coincide is further evidence that Haskell is indeed a good fit for such a task. \par 
As any software engineer will tell you, having a firm understanding of mathematical logic can be helpful in writing and debugging code. Boolean algebra is essential for the standard control flow mechanisms used in all languages. De Morgan's laws and other properties of first order logic can assist in cleaning up messy functions. It is well-known that a familiarity with combinatorics and graph theory can be helpful for those writing abstract data types or analyzing networks, and linear algebra and multivariate calculus are essential for working with 3D graphics. However, abstract algebra and category theory are rarely covered in the standard curriculum for a computer science degree. \par 
When using Haskell, a working knowledge of algebraic group theory and ring theory, while not strictly necessary, can provide useful insight into why certain typeclasses and functions behave the way they do. When writing a CAS, this recommendation is upgraded to a strict requirement. Even more central in the design of Haskell and other functionally minded languages is the presence of the ever-intimidating category theory. Even by theoretical mathematicians, this subject is often referred to as \textit{abstract nonsense}\cite{Saunders97}. Having a reputation as as being among the most abstruse of mathematical topics, the mere fact that something as concrete and useful as writing software could employ such a concept is itself remarkable. \par 
Using such a mathematically inclined language to implement a CAS is clearly an endeavour that requires a thorough understanding of the mathematics at play. Of particular importance is the theory of polynomials. 

\section{Polynomials}
Polynomials in a single variable with real coefficients are a familiar object of study, not only to professional mathematicians, but to anyone who has taken a high school math class. They are easy to understand and work with. Finding their roots, taking their derivatives, and analyzing their graphs are among the easier tasks a mathematician will attempt. We even estimate arbitrary analytic functions as polynomials via Taylor's theorem. Many of the algorithms that form the backbone of low-level software are based on this idea. It should come as no surprise then that generalizing these familiar objects is a popular practice. Using fields other than $\bR$ is a natural first step. In fact, when using an algebraically closed field like $\bC$, polynomials behave even more nicely than they did over $\bR$ as we don't have to wonder how many roots a degree $n$ polynomial may have; the answer is always $n$ (up to multiplicity). \par 
The next natural generalization is to allow for multiple (though still finitely many) variables. This generalization does introduce some complexity. For one thing, the idea of ordering the terms, which was taken for granted in the single variable case, now becomes a nontrivial discussion. The tools from multivariate calculus can be helpful in understanding the analytic nature of multivariate polynomials, while an introductory course in abstract algebra or algebraic geometry often addresses the more algebraic concerns by introducing term orders, algorithms, and the relationship between the ideals containing these objects and the affine varieties they generate. 

\subsection{Monomials}
There are two competing ways to view a polynomial. The first is as a function from some ring into itself. It is this view that is usually encountered first, and for most analytic purposes, this is sufficient. The other way is as a formal linear combination of indeterminates. This is the view that we prefer in this discussion. These indeterminates comprise a free monoid\footnote{Recall that a monoid is a set with an associative binary operation and an identity element. Being free means there are no unforced relationships between elements. For example, $x \ne y^n$ for any $n$.}, and (at least for now) we may assume the monoidal operation to be commutative. We use the term \textit{monomial} when referring to elements of a free monoid in the context of polynomials. \par 
In this view, the phrase \textit{polynomial in n variables} really means a linear combination of monomials drawn from a free monoid with $n$ distinct generators. Each generator is called a variable. When dealing with polynomials in a single variable, we usually use the symbol $x$ to refer to the generator of this monoid. So the polynomial $f$ defined as 
\[ f(x) = x^2-8x+15 \]
is a formal linear combination of the monomials $1$, $x$, and $x^2$ taken from the free monoid $\langle x \rangle$ with weights $1$,$-8$, and $15$. In the case of a three variable polynomial, the monoid used is generated by three elements. Here, there are two competing notational conventions. The first is to denote the generators as $x$, $y$, and $z$. The second is to denote the generators as $x_1$, $x_2$, and $x_3$. The former is often more readable, but the latter more easily generalizes to $n$ variables. \par 
This viewpoint of polynomials as a formal combination of monomials rather than a function may be summed up in the following two definitions courtesy of Cox, Little and O'Shea\cite{Cox2015}: 
\begin{definition}
  A \textbf{monomial} in $x_1,\dots,x_n$ is a product of the form 
  \[ x_1^{\alpha_1} \cdot x_2^{\alpha_2} \cdots x_n^{\alpha_n}, \]
  where all of the exponents $\alpha_1,\dots,\alpha_n$ are nonnegative integers. The \textbf{total degree} of this monomial is the sum $\alpha_1 + \dots + \alpha_n$.
\end{definition}
We can simplify the notation for monomials as follows: let $\alpha = (\alpha_1,\dots,\alpha_n)$ be an $n$-tuple of nonnegative integers. Then we set
\[ x^\alpha = x_1^{\alpha_1} \cdot x_2^{\alpha_2} \cdots x_n^{\alpha_n}. \]
When $\alpha=(0,\dots,0)$, note that $x^\alpha=1$. 
\begin{definition}
  A \textbf{polynomial} $f$ in $x_1,\dots,x_n$ with coefficients in a field $k$ is a finite linear combination (with coefficients in $k$) of monomials. We will write a polynomial $f$ in the form
  \[ f = \sum_\alpha a_\alpha x^\alpha,\quad a_\alpha \in k, \]
  where the sum is over a finite number of $n$-tuples $\alpha = (\alpha_1,\dots,\alpha_n)$. The set of all polynomials in $x_1,\dots,x_n$ with coefficients in $k$ is denoted $k[x_1,\dots,x_n]$. 
\end{definition}
\lipsum[3-10]

\subsection{Gr\"obner bases}
% Central to this discussion is the notion of a \emph{Gr\"obner basis}. Given an ideal of a polynomial ring, a Gr\"obner basis is a special set of polynomials which, when used as divisors in the division algorithm, give a unique remainder for any dividend in the ideal. \par 
\lipsum[11-25]

\section{Categories}
\lipsum[26-35]

\chapter{Functional Programming}
\lipsum[36-45]
% Among the key features of a language like Haskell is the guarantee of functional purity. A function in Haskell does exactly one thing. It returns a value. That's it. It cannot alter the state of the program. It cannot cause anything to happen other than returning that value. This makes a Haskell program far more predictable than any program written outside of a purely functional paradigm. This is particularly useful when writing multithreaded code. The inability for a thread to alter any resource that another thread is using simplifies the task tremendously. \par 
% Another benefit (at least for mathematicians) is how closely Haskell syntax resembles the syntax of mathematics. 

\section{Types}
\lipsum[46-55]

\section{Kinds}
\lipsum[56-65]

\section{The Polynomial Type}
\lipsum[66-75]

\chapter{Algorithms}
\lipsum[76-85]

\section{The Division Algorithm}
\lipsum[86-95]

\section{Buchberger's Algorithm}
\lipsum[96-105]
\cite{Cox2015}

\section{Efficiency}
\lipsum[106-115]

\chapter{Conclusion}
\lipsum[116-125]

\bibliographystyle{amsplain}
\bibliography{references}

\addcontentsline{toc}{chapter}{Curriculum Vitae}

\end{document}